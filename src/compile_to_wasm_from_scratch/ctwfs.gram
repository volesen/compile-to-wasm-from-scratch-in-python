@subheader '''
import traceback
from typing import Generator
from pegen.tokenizer import Tokenizer
import compile_to_wasm_from_scratch.ast as ast
'''

@trailer '''
import tokenize

def parse(source_file):
    tokens = tokenize.generate_tokens(source_file.readline)
    # We reuse Python's tokenizer, but remove indentation tokens
    filtered_tokens = (
        token
        for token in tokens
        if token.type not in {tokenize.NEWLINE, tokenize.INDENT, tokenize.DEDENT}
    )
    tokenizer = Tokenizer(filtered_tokens)
    parser = GeneratedParser(tokenizer)

    tree = parser.start()

    if not tree:
        err = parser.make_syntax_error(file.name)
        traceback.print_exception(err.__class__, err, None)

    return tree
'''

start: decls=decl* ENDMARKER { ast.Program(decls or []) }

decl: 'fn' NAME '(' params = params? ')' '=' expr { ast.FunctionDeclaration(name.string, params or [], expr) }

params: NAME names=(',' NAME)* { [name.string] + names }

expr: ( 'if' condition=expr 'then' then_branch=expr 'else' else_branch=expr { ast.If(condition, then_branch, else_branch) }
      | 'let' NAME '=' value=expr 'in' body=expr { ast.Let(name.string, value, body) }
      | comparison ';' expr { ast.Sequence(comparison, expr) }
      | comparison )

comparison: ( term op='=' comparison { ast.BinaryOp(op.string, term, comparison) }
            | term )

term: ( factor op=('+' | '-') term { ast.BinaryOp(op.string, factor, term) }
        | factor )

factor: ( unary op=('*' | '/') factor { ast.BinaryOp(op.string, unary, factor) }
        | unary )

unary: ( op='-' unary { ast.UnaryOp(op.string, unary) }
       | primary )

primary: ( NUMBER { ast.Number(int(number.string)) }
         | NAME '(' args = args? ')' { ast.Call(name.string, args or []) }
         | NAME { ast.Variable(name.string) }
         | '(' expr ')' { expr } )

args: expr exprs=(',' expr)* { [expr] + exprs }
